{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6526ae-d074-4bf5-a398-a0757bb96037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for local\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3090e9cc-79ea-49fe-951e-014d0cabdeee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Directory settings\n",
    "# ====================================================\n",
    "import os\n",
    "\n",
    "EXP_NAME='1020_lstm+gru_feat11'\n",
    "\n",
    "DATA_DIR = \"../input/ventilator-pressure-prediction/\"\n",
    "\n",
    "OUTPUT_DIR = f'./results/{EXP_NAME}/'\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c869b52-21c3-4379-ad34-3742feed0436",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbd9b558-02d5-463e-88d7-6c2abf835763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    experiment_name=EXP_NAME\n",
    "    competition='ventilator'\n",
    "    apex=True\n",
    "    print_freq=1000\n",
    "    num_workers=4\n",
    "    model_name='lstm+gru'\n",
    "    scheduler='ReduceLROnPlateau' # ['linear', 'cosine', 'ReduceLROnPlateau', 'CosineAnnealingLR', 'CosineAnnealingWarmRestarts']\n",
    "    batch_scheduler=False\n",
    "    #num_warmup_steps=100 # ['linear', 'cosine']\n",
    "    #num_cycles=0.5 # 'cosine'\n",
    "    factor=0.75 # ReduceLROnPlateau\n",
    "    patience=10 # ReduceLROnPlateau\n",
    "    eps=1e-8 # ReduceLROnPlateau\n",
    "    T_max=50 # CosineAnnealingLR\n",
    "    T_0=20 # CosineAnnealingWarmRestarts\n",
    "    epochs=300\n",
    "    max_grad_norm=1000\n",
    "    gradient_accumulation_steps=1\n",
    "    hidden_size=1024\n",
    "    lr=1e-3\n",
    "    min_lr=1e-5\n",
    "    weight_decay=1e-6\n",
    "    batch_size=256\n",
    "    n_fold=5\n",
    "    trn_fold=[0, 1, 2, 3, 4]\n",
    "    cate_seq_cols=[]\n",
    "    cont_seq_cols=['R', 'C', 'time_step', 'u_in', 'u_out']\n",
    "    train=True\n",
    "    inference=True\n",
    "    feature_importance=True\n",
    "    debug=False\n",
    "    wandb=True\n",
    "\n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold=[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aafe76b-c184-4572-b9e6-60c75e8566a7",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb48c0c6-056e-4d6a-b57e-eb125436de58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.cuda.amp import GradScaler\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#if CFG.apex:\n",
    "#    from apex import amp\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5889b437-33b9-420c-bd6e-e3dfea18814d",
   "metadata": {},
   "source": [
    "# wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd2c8248-73c8-463d-8b54-cbaaea65405a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mhypknot\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.5 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\n",
      "CondaEnvException: Unable to determine environment\n",
      "\n",
      "Please re-run this command with one of the following options:\n",
      "\n",
      "* Provide an environment name via --name or -n\n",
      "* Re-run this command inside an activated conda environment.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    Syncing run <strong><a href=\"https://wandb.ai/hypknot/Ventilator-Pressure-Public/runs/1oxlw339\" target=\"_blank\">graceful-spaceship-117</a></strong> to <a href=\"https://wandb.ai/hypknot/Ventilator-Pressure-Public\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
       "\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    import wandb\n",
    "\n",
    "    # try:\n",
    "    #     from kaggle_secrets import UserSecretsClient\n",
    "    #     user_secrets = UserSecretsClient()\n",
    "    #     secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "    #     wandb.login(key=secret_value_0)\n",
    "    #     anony = None\n",
    "    # except:\n",
    "    #     anony = \"must\"\n",
    "    #     print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "    anony=None # not for kaggle kernel\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project=\"Ventilator-Pressure-Public\", \n",
    "                     # name=CFG.model_name,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.experiment_name,\n",
    "                     job_type=\"train\",\n",
    "                     anonymous=anony)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed9123-6a9b-48b1-ba69-16a5232abc8c",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f30e1f12-d9f4-4155-96b9-549830918086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def get_score(y_trues, y_preds):\n",
    "    score = mean_absolute_error(y_trues, y_preds)\n",
    "    return score\n",
    "\n",
    "\n",
    "def init_logger(log_file=OUTPUT_DIR+'train.log'):\n",
    "    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=log_file)\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = init_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything()\n",
    "\n",
    "def decorate(s: str, decoration=None):\n",
    "    if decoration is None:\n",
    "        decoration = '★' * 20\n",
    "\n",
    "    return ' '.join([decoration, str(s), decoration])\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None, sep=' ', verbose=0):\n",
    "\n",
    "        if prefix: format_str = str(prefix) + sep + format_str\n",
    "        if suffix: format_str = format_str + sep + str(suffix)\n",
    "        self.format_str = format_str\n",
    "        self.logger = logger\n",
    "        self.start = None\n",
    "        self.end = None\n",
    "        self.verbose = verbose\n",
    "\n",
    "    @property\n",
    "    def duration(self):\n",
    "        if self.end is None:\n",
    "            return 0\n",
    "        return self.end - self.start\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end = time()\n",
    "        if self.verbose is None:\n",
    "            return\n",
    "        out_str = self.format_str.format(self.duration)\n",
    "        if self.logger:\n",
    "            self.logger.info(out_str)\n",
    "        else:\n",
    "            print(out_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93e67c7-9ec3-40a5-accf-ec78bad7b1e6",
   "metadata": {},
   "source": [
    "# data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5473c8e-d8a0-4caf-a1bf-fcb7e6417831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.083334</td>\n",
       "      <td>0</td>\n",
       "      <td>5.837492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.033652</td>\n",
       "      <td>18.383041</td>\n",
       "      <td>0</td>\n",
       "      <td>5.907794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.067514</td>\n",
       "      <td>22.509278</td>\n",
       "      <td>0</td>\n",
       "      <td>7.876254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.101542</td>\n",
       "      <td>22.808822</td>\n",
       "      <td>0</td>\n",
       "      <td>11.742872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>50</td>\n",
       "      <td>0.135756</td>\n",
       "      <td>25.355850</td>\n",
       "      <td>0</td>\n",
       "      <td>12.234987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id   R   C  time_step       u_in  u_out   pressure\n",
       "0   1          1  20  50   0.000000   0.083334      0   5.837492\n",
       "1   2          1  20  50   0.033652  18.383041      0   5.907794\n",
       "2   3          1  20  50   0.067514  22.509278      0   7.876254\n",
       "3   4          1  20  50   0.101542  22.808822      0  11.742872\n",
       "4   5          1  20  50   0.135756  25.355850      0  12.234987"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>breath_id</th>\n",
       "      <th>R</th>\n",
       "      <th>C</th>\n",
       "      <th>time_step</th>\n",
       "      <th>u_in</th>\n",
       "      <th>u_out</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.031904</td>\n",
       "      <td>7.515046</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.063827</td>\n",
       "      <td>14.651675</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.095751</td>\n",
       "      <td>21.230610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0.127644</td>\n",
       "      <td>26.320956</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  breath_id  R   C  time_step       u_in  u_out\n",
       "0   1          0  5  20   0.000000   0.000000      0\n",
       "1   2          0  5  20   0.031904   7.515046      0\n",
       "2   3          0  5  20   0.063827  14.651675      0\n",
       "3   4          0  5  20   0.095751  21.230610      0\n",
       "4   5          0  5  20   0.127644  26.320956      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pressure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  pressure\n",
       "0   1         0\n",
       "1   2         0\n",
       "2   3         0\n",
       "3   4         0\n",
       "4   5         0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Data Loading\n",
    "# ====================================================\n",
    "train = pd.read_csv(DATA_DIR + 'train.csv')\n",
    "if CFG.debug:\n",
    "    train = train[:80*5000]\n",
    "test = pd.read_csv(DATA_DIR + 'test.csv')\n",
    "sub = pd.read_csv(DATA_DIR + 'sample_submission.csv')\n",
    "\n",
    "display(train.head())\n",
    "display(test.head())\n",
    "display(sub.head())\n",
    "\n",
    "unique_pressures = train[\"pressure\"].unique()\n",
    "sorted_pressures = np.sort(unique_pressures)\n",
    "total_pressures_len = len(sorted_pressures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2894d-a8c0-4dd8-b442-3f6b6a2a1740",
   "metadata": {},
   "source": [
    "# create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e894260-32d0-4b9d-a9b6-fd7daf9bf609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractBaseBlock:\n",
    "    def fit(self, input_df: pd.DataFrame, y=None):\n",
    "        return self.transform(input_df)\n",
    "\n",
    "    def transform(self, input_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "\n",
    "class AddMultiplyingDividing(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        input_df['area'] = input_df['time_step'] * input_df['u_in']\n",
    "        input_df['area'] = input_df.groupby('breath_id')['area'].cumsum()\n",
    "        input_df['cross'] = input_df['u_in']*input_df['u_out']\n",
    "        input_df['cross2'] = input_df['time_step']*input_df['u_out']\n",
    "        input_df['u_in_cumsum'] = (input_df['u_in']).groupby(input_df['breath_id']).cumsum()\n",
    "        input_df['one'] = 1\n",
    "        input_df['count'] = (input_df['one']).groupby(input_df['breath_id']).cumsum()\n",
    "        input_df['u_in_cummean'] = input_df['u_in_cumsum'] / input_df['count']\n",
    "        # input_df = input_df.merge(\n",
    "        #     input_df[input_df[\"u_out\"]==0].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out0_\").reset_index(),\n",
    "        #     on=\"breath_id\"\n",
    "        # )\n",
    "        # input_df = input_df.merge(\n",
    "        #     input_df[input_df[\"u_out\"]==1].groupby('breath_id')['u_in'].agg([\"mean\", \"std\", \"max\"]).add_prefix(\"u_out1_\").reset_index(),\n",
    "        #     on=\"breath_id\"\n",
    "        # )\n",
    "\n",
    "        # feat-11\n",
    "        input_df['time_step_cumsum'] = input_df.groupby(['breath_id'])['time_step'].cumsum()\n",
    "        input_df['breath_id__u_in__max'] = input_df.groupby(['breath_id'])['u_in'].transform('max')\n",
    "        input_df['breath_id__u_in__mean'] = input_df.groupby(['breath_id'])['u_in'].transform('mean')\n",
    "        input_df['breath_id__u_in__diffmax'] = input_df.groupby(['breath_id'])['u_in'].transform('max') - input_df['u_in']\n",
    "        input_df['breath_id__u_in__diffmean'] = input_df.groupby(['breath_id'])['u_in'].transform('mean') - input_df['u_in']\n",
    "\n",
    "        output_df = pd.DataFrame(\n",
    "            {\n",
    "                \"area\": input_df['area'],\n",
    "                #\"cross\": input_df['cross'],\n",
    "                #\"cross2\": input_df['cross2'],\n",
    "                \"u_in_cumsum\": input_df['u_in_cumsum'],\n",
    "                \"u_in_cummean\": input_df['u_in_cummean'],\n",
    "                'time_step_cumsum': input_df['time_step_cumsum'],\n",
    "                \"breath_id__u_in__max\": input_df['breath_id__u_in__max'],\n",
    "                \"breath_id__u_in__mean\": input_df['breath_id__u_in__mean'],\n",
    "                \"breath_id__u_in__diffmax\": input_df['breath_id__u_in__diffmax'],\n",
    "                \"breath_id__u_in__diffmean\": input_df['breath_id__u_in__diffmean'],\n",
    "\n",
    "            }\n",
    "        )\n",
    "        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class RCDummry(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        input_df['R_dummy'] = input_df['R'].astype(str)\n",
    "        input_df['C_dummy'] = input_df['C'].astype(str)\n",
    "        #input_df['RC_dummy'] = input_df['R_dummy'] + input_df['C_dummy']\n",
    "        output_df = pd.get_dummies(input_df[[\"R_dummy\", \"C_dummy\"]])\n",
    "        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class AddBreathTimeAndUInTime(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame(\n",
    "            {\n",
    "                \"breath_time\": input_df['time_step'] - input_df['time_step'].shift(1),\n",
    "                \"u_in_time\": input_df['u_in'] - input_df['u_in'].shift(1)\n",
    "            }\n",
    "        )\n",
    "        output_df.loc[input_df['time_step'] == 0, 'breath_time'] = output_df['breath_time'].mean()\n",
    "        output_df.loc[input_df['time_step'] == 0, 'u_in_time'] = output_df['u_in_time'].mean()\n",
    "        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n",
    "        return output_df\n",
    "\n",
    "class LagFeatures(AbstractBaseBlock):\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame(\n",
    "            {\n",
    "                \"u_in_lag1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(1).fillna(0),\n",
    "                \"u_in_lag2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(2).fillna(0),\n",
    "                \"u_in_lag3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(3).fillna(0),\n",
    "                \"u_in_lag4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(4).fillna(0),\n",
    "                \"u_in_lag-1\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-1).fillna(0),\n",
    "                \"u_in_lag-2\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-2).fillna(0),\n",
    "                \"u_in_lag-3\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-3).fillna(0),\n",
    "                \"u_in_lag-4\": input_df.groupby(\"breath_id\")[\"u_in\"].shift(-4).fillna(0),\n",
    "                \"u_out_lag1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(1).fillna(0),\n",
    "                \"u_out_lag2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(2).fillna(0),\n",
    "                \"u_out_lag3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(3).fillna(0),\n",
    "                \"u_out_lag4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(4).fillna(0),\n",
    "                \"u_out_lag-1\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-1).fillna(0),\n",
    "                \"u_out_lag-2\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-2).fillna(0),\n",
    "                \"u_out_lag-3\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-3).fillna(0),\n",
    "                \"u_out_lag-4\": input_df.groupby(\"breath_id\")[\"u_out\"].shift(-4).fillna(0),\n",
    "            }\n",
    "        )\n",
    "        output_df[\"u_in_lag1_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag1\"]\n",
    "        output_df[\"u_in_lag2_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag2\"]\n",
    "        output_df[\"u_in_lag3_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag3\"]\n",
    "        output_df[\"u_in_lag4_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag4\"]\n",
    "        output_df[\"u_out_lag1_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag1\"]\n",
    "        output_df[\"u_out_lag2_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag2\"]\n",
    "        output_df[\"u_out_lag3_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag3\"]\n",
    "        output_df[\"u_out_lag4_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag4\"]\n",
    "        output_df[\"u_in_lag-1_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag-1\"]\n",
    "        output_df[\"u_in_lag-2_diff\"] = input_df[\"u_in\"] - output_df[\"u_in_lag-2\"]\n",
    "        output_df[\"u_out_lag-1_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag-1\"]\n",
    "        output_df[\"u_out_lag-2_diff\"] = input_df[\"u_out\"] - output_df[\"u_out_lag-2\"]\n",
    "\n",
    "        output_df['u_in_ewm9'] = (input_df.groupby('breath_id')['u_in'].ewm(halflife=9).mean().reset_index(level=0,drop=True))\n",
    "        output_df['u_in_ewm15'] = (input_df.groupby('breath_id')['u_in'].ewm(halflife=15).mean().reset_index(level=0,drop=True))\n",
    "\n",
    "        output_df[\"u_in_rolling_mean2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).mean()[\"u_in\"].reset_index(drop=True)\n",
    "        output_df[\"u_in_rolling_mean4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).mean()[\"u_in\"].reset_index(drop=True)\n",
    "        output_df[\"u_in_rolling_mean15\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(15).mean()[\"u_in\"].reset_index(drop=True)\n",
    "        if not CFG.debug:\n",
    "            output_df[\"u_in_rolling_max2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).max()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_max4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).max()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_max15\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(15).max()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_min2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).min()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_min4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).min()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_min15\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(15).min()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_std2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).std()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_std4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).std()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_std15\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(15).std()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_sum2\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(2).sum()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_sum4\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(4).sum()[\"u_in\"].reset_index(drop=True)\n",
    "            output_df[\"u_in_rolling_sum15\"] = input_df[[\"breath_id\", \"u_in\"]].groupby(\"breath_id\").rolling(15).sum()[\"u_in\"].reset_index(drop=True)\n",
    "        for col in output_df.columns:\n",
    "            output_df[col] = output_df[col].fillna(output_df[col].mean())\n",
    "        CFG.cont_seq_cols += output_df.add_suffix(f'@{self.__class__.__name__}').columns.tolist()\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3418b5a4-23eb-41dd-933e-05410ff5388f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_blocks = [\n",
    "    AddMultiplyingDividing(),\n",
    "    AddBreathTimeAndUInTime(),\n",
    "    RCDummry(),\n",
    "    LagFeatures()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac73845-a919-4c3a-b854-8cc4b5e5b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "★★★★★★★★★★★★★★★★★★★★ start run blocks... ★★★★★★★★★★★★★★★★★★★★\n",
      "out_df shape: (0, 0) \t- <__main__.AddMultiplyingDividing object at 0x7f1c9318a9d0> 1.837[s]\n",
      "out_df shape: (6036000, 8) \t- <__main__.AddBreathTimeAndUInTime object at 0x7f1c9318aa60> 0.287[s]\n",
      "out_df shape: (6036000, 10) \t- <__main__.RCDummry object at 0x7f1c9318aa00> 10.375[s]\n"
     ]
    }
   ],
   "source": [
    "def run_blocks(input_df, blocks, y=None, test=False):\n",
    "    out_df = pd.DataFrame()\n",
    "\n",
    "    print(decorate('start run blocks...'))\n",
    "\n",
    "    with Timer(prefix='run test={}'.format(test)):\n",
    "        for block in feature_blocks:\n",
    "            with Timer(prefix='out_df shape: {} \\t- {}'.format(out_df.shape, str(block))):\n",
    "                if not test:\n",
    "                    out_i = block.fit(input_df.copy(), y=y)\n",
    "                else:\n",
    "                    out_i = block.transform(input_df.copy())\n",
    "\n",
    "            assert len(input_df) == len(out_i), block\n",
    "            name = block.__class__.__name__\n",
    "            out_df = pd.concat([out_df, out_i.add_suffix(f'@{name}')], axis=1)\n",
    "    print(f\"out_df shape: {out_df.shape}\")\n",
    "\n",
    "    return pd.concat([input_df, out_df], axis=1)\n",
    "\n",
    "train = run_blocks(train, blocks=feature_blocks)\n",
    "test = run_blocks(test, blocks=feature_blocks, test=True)\n",
    "CFG.cont_seq_cols = list(set(CFG.cont_seq_cols))\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2480b081-f985-4f16-bb51-5748f27d437c",
   "metadata": {},
   "source": [
    "# normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b5117-dfdf-4a38-ad1e-1c16b60e1268",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_col_order = [\"u_out\"] + train.columns.drop(\"u_out\").tolist()\n",
    "test_col_order = [\"u_out\"] + test.columns.drop(\"u_out\").tolist()\n",
    "train = train[train_col_order]\n",
    "test = test[test_col_order]\n",
    "scaler = RobustScaler()\n",
    "scaler_targets = [col for col in CFG.cont_seq_cols if col != \"u_out\"]\n",
    "print(f\"Apply Standerd Scaler these columns: {scaler_targets}\")\n",
    "for scaler_target in tqdm(scaler_targets):\n",
    "    scaler.fit(train.loc[:,[scaler_target]])\n",
    "    train.loc[:,[scaler_target]] = scaler.transform(train.loc[:,[scaler_target]])\n",
    "    test.loc[:,[scaler_target]] = scaler.transform(test.loc[:,[scaler_target]])\n",
    "display(train.head())\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf89c15-d112-42b2-92f6-9135dbbe7177",
   "metadata": {},
   "source": [
    "# reshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf170ebc-296b-4e1e-8ef9-2b31eb771596",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).columns) - set(CFG.cont_seq_cols))\n",
    "print(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1).shape)\n",
    "print(len(CFG.cont_seq_cols))\n",
    "\n",
    "X = np.float32(train.drop([\"id\", \"breath_id\", \"pressure\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))\n",
    "y = np.float32(train[\"pressure\"]).reshape(-1, 80, 1)\n",
    "X_test = np.float32(test.drop([\"id\", \"breath_id\"], axis=1)).reshape(-1, 80, len(CFG.cont_seq_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cd6dd6-27c0-4499-a1f4-8ac582238df7",
   "metadata": {},
   "source": [
    "# cv split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a9e8779-c77b-457c-8d65-63888cb98327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "# Fold = GroupKFold(n_splits=5)\n",
    "# groups = train['breath_id'].values\n",
    "# for n, (train_index, val_index) in enumerate(Fold.split(train, train['pressure'], groups)):\n",
    "#     train.loc[val_index, 'fold'] = int(n)\n",
    "# train['fold'] = train['fold'].astype(int)\n",
    "# print(train.groupby('fold').size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed2f72-9b32-472f-a32f-bfab28b02e0b",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348def8c-85cc-4bba-96dd-4c068795ea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Dataset\n",
    "# ====================================================\n",
    "# class TrainDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "#         self.groups = df.groupby('breath_id').groups\n",
    "#         self.keys = list(self.groups.keys())\n",
    "#         \n",
    "#     def __len__(self):\n",
    "#         return len(self.groups)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         indexes = self.groups[self.keys[idx]]\n",
    "#         df = self.df.iloc[indexes]\n",
    "#         cont_seq_x = torch.FloatTensor(df[CFG.cont_seq_cols].values)\n",
    "#         u_out = torch.LongTensor(df['u_out'].values)\n",
    "#         label = torch.FloatTensor(df['pressure'].values)\n",
    "#         return cont_seq_x, u_out, label\n",
    "#     \n",
    "# \n",
    "# class TestDataset(Dataset):\n",
    "#     def __init__(self, df):\n",
    "#         self.df = df\n",
    "#         self.groups = df.groupby('breath_id').groups\n",
    "#         self.keys = list(self.groups.keys())\n",
    "#         \n",
    "#     def __len__(self):\n",
    "#         return len(self.groups)\n",
    "# \n",
    "#     def __getitem__(self, idx):\n",
    "#         indexes = self.groups[self.keys[idx]]\n",
    "#         df = self.df.iloc[indexes]\n",
    "#         cont_seq_x = torch.FloatTensor(df[CFG.cont_seq_cols].values)\n",
    "#         return cont_seq_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ad8259-e25d-4455-ba22-f23326b4f656",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231f1614-9a8f-4db3-b8cf-177807cc1e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Loss_masked(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    def forward(self, preds, y, u_out):\n",
    "\n",
    "        mask = 1 - u_out\n",
    "        mae = torch.abs(mask * (y - preds))\n",
    "        mae = torch.sum(mae) / torch.sum(mask)\n",
    "\n",
    "        return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc35dbb6-3ce0-4a88-a7fd-2c7708f9838a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30daacd7-cb5b-40cc-8be2-5d29a7f52877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Model\n",
    "# ====================================================\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.hidden_size = cfg.hidden_size\n",
    "#         self.seq_emb = nn.Sequential(\n",
    "#             nn.Linear(len(cfg.cont_seq_cols), self.hidden_size),\n",
    "#             nn.LayerNorm(self.hidden_size),\n",
    "#             nn.GELU(),\n",
    "#             #nn.Dropout(0.1),\n",
    "#         )\n",
    "        self.lstm1 = nn.LSTM(len(cfg.cont_seq_cols), self.hidden_size//4 * 3, batch_first=True, bidirectional=True) # output_size=768 (bi=1536)\n",
    "        self.lstm2 = nn.LSTM(self.hidden_size//4 * 3 * 2, self.hidden_size//2, batch_first=True, bidirectional=True)\n",
    "        self.lstm3 = nn.LSTM(self.hidden_size//2 * 2, self.hidden_size//4, batch_first=True, bidirectional=True)\n",
    "        self.lstm4 = nn.LSTM(self.hidden_size//4 * 2, self.hidden_size//8, batch_first=True, bidirectional=True) \n",
    "        self.gru1 = nn.GRU(self.hidden_size//2 * 2, self.hidden_size//4, batch_first=True, bidirectional=True)\n",
    "        self.gru2 = nn.GRU(self.hidden_size//4 * 2, self.hidden_size//8, batch_first=True, bidirectional=True)\n",
    "        self.gru3 = nn.GRU(self.hidden_size//8 * 2, self.hidden_size//16, batch_first=True, bidirectional=True)\n",
    "        fc_input_size = ((self.hidden_size//8 * 2) + (self.hidden_size//4 * 2) + (self.hidden_size//8 * 2) + (self.hidden_size//16 * 2))\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(fc_input_size),\n",
    "            nn.SELU(),\n",
    "            #nn.Dropout(0.),\n",
    "            nn.Linear(fc_input_size, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.SELU(),\n",
    "            #nn.Dropout(0.),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "        for n, m in self.named_modules():\n",
    "            if isinstance(m, nn.LSTM):\n",
    "                print(f'init {m}')\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        nn.init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        nn.init.normal_(param.data)\n",
    "            elif isinstance(m, nn.GRU):\n",
    "                print(f\"init {m}\")\n",
    "                for param in m.parameters():\n",
    "                    if len(param.shape) >= 2:\n",
    "                        init.orthogonal_(param.data)\n",
    "                    else:\n",
    "                        init.normal_(param.data)\n",
    "\n",
    "    def forward(self, cont_seq_x):\n",
    "        bs = cont_seq_x.size(0)\n",
    "        x1, _ = self.lstm1(cont_seq_x)\n",
    "        x2, _ = self.lstm2(x1)\n",
    "        x3, _ = self.lstm3(x2)\n",
    "        x4, _ = self.lstm4(x3)\n",
    "        z2, _ = self.gru1(x2)\n",
    "        z3, _ = self.gru2(x3+z2)\n",
    "        z4, _ = self.gru3(x4+z3)\n",
    "        output = self.head(torch.cat((x4, z2, z3, z4), dim=2))\n",
    "        return output\n",
    "print(CustomModel(CFG))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd76d6-fe03-40ee-b27b-7df4a2bbab36",
   "metadata": {},
   "source": [
    "# helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e1185c-1c79-4240-9022-4c80a5de62c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# helper function\n",
    "# ====================================================\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n",
    "\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "def train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n",
    "    model.train()\n",
    "    losses = AverageMeter()\n",
    "    start = end = time()\n",
    "    for step, (inputs, y) in enumerate(train_loader):\n",
    "        inputs, y = inputs.to(device), y.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        with autocast():\n",
    "            pred = model(inputs)\n",
    "            loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        if CFG.apex:\n",
    "            scaler.scale(loss).backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)\n",
    "        if (step + 1) % CFG.gradient_accumulation_steps == 0:\n",
    "            if CFG.apex:\n",
    "                scaler.step(optimizer)\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            lr = 0\n",
    "            if CFG.batch_scheduler:\n",
    "                scheduler.step()\n",
    "                lr = scheduler.get_lr()[0]\n",
    "        if CFG.apex:\n",
    "            scaler.update()\n",
    "        end = time()\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] loss\": losses.val, \n",
    "                       f\"[fold{fold}] lr\": optimizer.param_groups[0]['lr']})\n",
    "    return losses.avg\n",
    "\n",
    "\n",
    "def valid_fn(valid_loader, model, criterion, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    losses = AverageMeter()\n",
    "    start = end = time()\n",
    "    for step, (inputs, y) in enumerate(valid_loader):\n",
    "        inputs, y = inputs.to(device), y.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputs)\n",
    "        loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "        end = time()\n",
    "    preds = np.concatenate(preds)\n",
    "    return losses.avg, preds\n",
    "\n",
    "\n",
    "def inference_fn(test_loader, model, device):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    preds = []\n",
    "    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n",
    "    for step, (cont_seq_x) in tk0:\n",
    "        cont_seq_x = cont_seq_x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(cont_seq_x)\n",
    "        preds.append(pred.view(-1).detach().cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    return preds\n",
    "\n",
    "def find_nearest(prediction):\n",
    "    '''\n",
    "    予測値は離散値であるため、学習データにある最も近い離散値に置き換える\n",
    "    '''\n",
    "    insert_idx = np.searchsorted(sorted_pressures, prediction)\n",
    "    if insert_idx == total_pressures_len:\n",
    "        # If the predicted value is bigger than the highest pressure in the train dataset,\n",
    "        # return the max value.\n",
    "        return sorted_pressures[-1]\n",
    "    elif insert_idx == 0:\n",
    "        # Same control but for the lower bound.\n",
    "        return sorted_pressures[0]\n",
    "    lower_val = sorted_pressures[insert_idx - 1]\n",
    "    upper_val = sorted_pressures[insert_idx]\n",
    "    return lower_val if abs(lower_val - prediction) < abs(upper_val - prediction) else upper_val\n",
    "\n",
    "def feature_importance_fn(X_valid, y_valid, model, criterion, device):\n",
    "    valid_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(X_valid),\n",
    "        torch.from_numpy(y_valid)\n",
    "    )\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    losses = AverageMeter()\n",
    "    for step, (inputs, y) in enumerate(valid_loader):\n",
    "        inputs, y = inputs.to(device), y.to(device)\n",
    "        batch_size = inputs.size(0)\n",
    "        with torch.no_grad():\n",
    "            pred = model(inputs)\n",
    "        loss = criterion(pred, y, inputs[:,:,0].reshape(-1,80,1))\n",
    "        losses.update(loss.item(), batch_size)\n",
    "        if CFG.gradient_accumulation_steps > 1:\n",
    "            loss = loss / CFG.gradient_accumulation_steps\n",
    "    return losses.avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af77e2d-2ded-44c5-ad5a-069a81f7f3d6",
   "metadata": {},
   "source": [
    "# Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487dd41a-ec3f-4cfc-8f20-7cc012c33556",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"breath_id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4697430b-06c7-482c-af27-31db3f6f7858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# train loop\n",
    "# ====================================================\n",
    "def train_loop(folds, fold, trn_idx, val_idx):\n",
    "\n",
    "    LOGGER.info(f\"========== fold: {fold} training ==========\")\n",
    "\n",
    "    # ====================================================\n",
    "    # loader\n",
    "    # ====================================================\n",
    "    #trn_idx = folds[folds['fold'] != fold].index\n",
    "    #val_idx = folds[folds['fold'] == fold].index\n",
    "    \n",
    "    train_folds = X[trn_idx]\n",
    "    valid_folds = X[val_idx]\n",
    "    groups = train[\"breath_id\"].unique()[val_idx]\n",
    "    oof_folds = train[train[\"breath_id\"].isin(groups)].reset_index(drop=True)\n",
    "    y_train = y[trn_idx]\n",
    "    y_true = y[val_idx]\n",
    "\n",
    "    # train_dataset = TrainDataset(train_folds)\n",
    "    # valid_dataset = TrainDataset(valid_folds)\n",
    "    train_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(train_folds),\n",
    "        torch.from_numpy(y_train)\n",
    "    )\n",
    "    valid_dataset = torch.utils.data.TensorDataset(\n",
    "        torch.from_numpy(valid_folds),\n",
    "        torch.from_numpy(y_true)\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=CFG.batch_size,\n",
    "                              shuffle=False,\n",
    "                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # ====================================================\n",
    "    # model & optimizer\n",
    "    # ====================================================\n",
    "    model = CustomModel(CFG)\n",
    "    model.to(device)\n",
    "\n",
    "    #optimizer = AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001, eps=1e-07)\n",
    "    num_train_steps = int(len(train_folds) / CFG.batch_size * CFG.epochs)\n",
    "    \n",
    "    def get_scheduler(optimizer):\n",
    "        if CFG.scheduler=='linear':\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps\n",
    "            )\n",
    "        elif CFG.scheduler=='cosine':\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer, num_warmup_steps=CFG.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=CFG.num_cycles\n",
    "            )\n",
    "        elif CFG.scheduler=='ReduceLROnPlateau':\n",
    "            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG.factor, patience=CFG.patience, verbose=True, eps=CFG.eps)\n",
    "        elif CFG.scheduler=='CosineAnnealingLR':\n",
    "            scheduler = CosineAnnealingLR(optimizer, T_max=CFG.T_max, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        elif CFG.scheduler=='CosineAnnealingWarmRestarts':\n",
    "            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG.T_0, T_mult=1, eta_min=CFG.min_lr, last_epoch=-1)\n",
    "        return scheduler\n",
    "\n",
    "    scheduler = get_scheduler(optimizer)\n",
    "\n",
    "    # ====================================================\n",
    "    # apex\n",
    "    # ====================================================\n",
    "    #if CFG.apex:\n",
    "    #    model, optimizer = amp.initialize(model, optimizer, opt_level='O1', verbosity=0)\n",
    "\n",
    "    # ====================================================\n",
    "    # loop\n",
    "    # ====================================================\n",
    "    criterion = L1Loss_masked()\n",
    "\n",
    "    best_score = np.inf\n",
    "\n",
    "    avg_losses = []\n",
    "    avg_val_losses = []\n",
    "    for epoch in range(CFG.epochs):\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        # train\n",
    "        avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n",
    "        #avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, None, device)\n",
    "        avg_losses.append(avg_loss)\n",
    "        \n",
    "        # eval\n",
    "        avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n",
    "        avg_val_losses.append(avg_val_loss)\n",
    "        \n",
    "        if isinstance(scheduler, ReduceLROnPlateau):\n",
    "            scheduler.step(avg_val_loss)\n",
    "        elif isinstance(scheduler, CosineAnnealingLR):\n",
    "            scheduler.step()\n",
    "        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n",
    "            scheduler.step()\n",
    "\n",
    "        # scoring\n",
    "        score = avg_val_loss #get_score(y_true[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "\n",
    "        elapsed = time() - start_time\n",
    "\n",
    "        best_notice = \"\"\n",
    "        if score < best_score:\n",
    "            best_notice = \"Best Score\"\n",
    "            best_score = score\n",
    "            # LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {score:.4f} Model')\n",
    "            torch.save({'model': model.state_dict(),\n",
    "                        'preds': preds},\n",
    "                        OUTPUT_DIR+f\"fold{fold}_best.pth\")\n",
    "        if CFG.wandb:\n",
    "            wandb.log({f\"[fold{fold}] epoch\": epoch+1, \n",
    "                       f\"[fold{fold}] avg_train_loss\": avg_loss, \n",
    "                       f\"[fold{fold}] avg_val_loss\": avg_val_loss,\n",
    "                       f\"[fold{fold}] score\": score,\n",
    "                       f\"[fold{fold}] best_score\": best_score})\n",
    "    \n",
    "        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s, lr: {optimizer.param_groups[0][\"lr\"]:.5f}, MAE Score: {score:.4f}, {best_notice}')\n",
    "\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(avg_losses, label=\"Train Loss\")\n",
    "    plt.plot(avg_val_losses, label=\"Train Loss\")\n",
    "    plt.title(f\"Fold {fold + 1} - Best score {best_score:.4f}\", size=18)\n",
    "    plt.show()\n",
    "\n",
    "    preds = torch.load(OUTPUT_DIR+f\"fold{fold}_best.pth\", map_location=torch.device('cpu'))['preds']\n",
    "    oof_folds['preds'] = preds.flatten()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return oof_folds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272d9dc8-48e0-48af-94ae-891b83c01544",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c61d23-e3c8-4ac5-b98d-5f8adfecb26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# main\n",
    "# ====================================================\n",
    "def main():\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepare: 1.train 2.test\n",
    "    \"\"\"\n",
    "    \n",
    "    def get_result(result_df):\n",
    "        preds = result_df['preds'].values\n",
    "        labels = result_df['pressure'].values\n",
    "        non_expiratory_phase_val_idx = result_df[result_df['u_out'] == 0].index # The expiratory phase is not scored\n",
    "        score = get_score(labels[non_expiratory_phase_val_idx], preds[non_expiratory_phase_val_idx])\n",
    "        LOGGER.info(f'Score (without expiratory phase): {score:<.4f}')\n",
    "    \n",
    "    if CFG.train:\n",
    "        # train \n",
    "        oof_df = pd.DataFrame()\n",
    "        kfold = KFold(n_splits=CFG.n_fold, random_state=42, shuffle=True)\n",
    "        for fold, (trn_idx, val_idx) in enumerate(kfold.split(X=X, y=y)):\n",
    "            if fold in CFG.trn_fold:\n",
    "                _oof_df = train_loop(X, fold, trn_idx, val_idx)\n",
    "                oof_df = pd.concat([oof_df, _oof_df])\n",
    "                LOGGER.info(f\"========== fold: {fold} result ==========\")\n",
    "                get_result(_oof_df)\n",
    "        # CV result\n",
    "        LOGGER.info(f\"========== CV ==========\")\n",
    "        get_result(oof_df)\n",
    "        # save result\n",
    "        oof_df.to_csv(OUTPUT_DIR+'oof_df.csv', index=False)\n",
    "        for i, breath_id in enumerate(oof_df[\"breath_id\"].unique()):\n",
    "            oof_df[oof_df[\"breath_id\"]==breath_id].plot(x=\"time_step\", y=[\"preds\", \"pressure\", \"u_out\"], figsize=(16, 5))\n",
    "            plt.show()\n",
    "            if i == 10:\n",
    "                break\n",
    "    \n",
    "    if CFG.inference:\n",
    "        test_loader = torch.utils.data.DataLoader(X_test, batch_size=512, shuffle=False, pin_memory=True)\n",
    "        #test_loader = DataLoader(test_dataset, batch_size=CFG.batch_size * 2, shuffle=False, num_workers=CFG.num_workers, pin_memory=True)\n",
    "        for fold in CFG.trn_fold:\n",
    "            model = CustomModel(CFG)\n",
    "            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            predictions = inference_fn(test_loader, model, device)\n",
    "            test[f'fold{fold}'] = predictions\n",
    "            del state, predictions; gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        # submission\n",
    "        test['pressure'] = test[[f'fold{fold}' for fold in CFG.trn_fold]].mean(1)\n",
    "        test['pressure'] = test['pressure'].apply(find_nearest)\n",
    "        test[['id', 'pressure']+[f'fold{fold}' for fold in CFG.trn_fold]].to_csv(OUTPUT_DIR+'raw_submission_mean.csv', index=False)\n",
    "        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission_mean.csv', index=False)\n",
    "        \n",
    "        test['pressure'] = test[[f'fold{fold}' for fold in CFG.trn_fold]].median(1)\n",
    "        test['pressure'] = test['pressure'].apply(find_nearest)\n",
    "        test[['id', 'pressure']+[f'fold{fold}' for fold in CFG.trn_fold]].to_csv(OUTPUT_DIR+'raw_submission_median.csv', index=False)\n",
    "        test[['id', 'pressure']].to_csv(OUTPUT_DIR+'submission_median.csv', index=False)\n",
    "        \n",
    "    if CFG.feature_importance:\n",
    "        fi_results = []\n",
    "        print('Computing LSTM feature importance...')\n",
    "        kfold = KFold(n_splits=CFG.n_fold, random_state=42, shuffle=True)\n",
    "        for fold, (trn_idx, val_idx) in enumerate(kfold.split(X=X, y=y)):\n",
    "            model = CustomModel(CFG)\n",
    "            path = OUTPUT_DIR+f\"fold{fold}_best.pth\"\n",
    "            state = torch.load(path, map_location=torch.device('cpu'))\n",
    "            model.load_state_dict(state['model'])\n",
    "            X_valid = X[val_idx]\n",
    "            y_valid = y[val_idx]\n",
    "            if fold in CFG.trn_fold: \n",
    "                for k in tqdm(range(len(CFG.cont_seq_cols))):\n",
    "                    criterion = L1Loss_masked()\n",
    "                    if k>0: \n",
    "                        save_col = X_valid[:,:,k-1].copy()\n",
    "                        np.random.shuffle(X_valid[:,:,k-1])\n",
    "                    \n",
    "                    avg_val_loss = feature_importance_fn(X_valid, y_valid, model, criterion, device)\n",
    "                    fi_results.append({'feature':CFG.cont_seq_cols[k],'avg_val_loss':avg_val_loss})\n",
    "\n",
    "                    if k>0: \n",
    "                        X_valid[:,:,k-1] = save_col\n",
    "            # compute feature importance with only one fold\n",
    "            break\n",
    "        # DISPLAY LSTM FEATURE IMPORTANCE\n",
    "        print()\n",
    "        fi_df = pd.DataFrame(fi_results)\n",
    "        fi_df = fi_df.sort_values('avg_val_loss')\n",
    "        fig, ax = plt.subplots(figsize=(10,20))\n",
    "        ax.barh(np.arange(len(CFG.cont_seq_cols)),fi_df.avg_val_loss)\n",
    "        plt.yticks(np.arange(len(CFG.cont_seq_cols)),fi_df.feature.values)\n",
    "        plt.title('LSTM Feature Importance',size=16)\n",
    "        plt.ylim((-1,len(CFG.cont_seq_cols)))\n",
    "        plt.show()\n",
    "        fig.savefig(OUTPUT_DIR+f'{CFG.model_name}_feature_imporance.png')\n",
    "\n",
    "        # SAVE LSTM FEATURE IMPORTANCE\n",
    "        fi_df = fi_df.sort_values('avg_val_loss',ascending=False)\n",
    "        fi_df.to_csv(OUTPUT_DIR+f'{CFG.model_name}_feature_importance_fold_{fold}.csv',index=False)\n",
    "    if CFG.wandb:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1960c-990b-4a2b-abc1-25e6f70b92f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe43e5c-9f77-491f-97d2-6be56ef68ffa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
